"""===============================================================================NeuroEng Voltage Transient Processor - Voltage Selector (Stage 2)===============================================================================Author:     Ifra Ilyas AnsariCreated:    2025-07-07Modified:   2025-10-30Code:       VT_Processor_Py_Stage2_VoltageSelectorVersion:    v1.7.0DESCRIPTION---------------------------This is **Stage 2** of the voltage transient processing pipeline.This script loads the `.pkl` dictionary file generated by Stage 1. It thenperforms the following operations:- **Loads** the dictionary of aligned DataFrames from the selected `.pkl` file.- **Loads** the `PyPD_Metadata_Loaded.CSV` file from the *same* Stage 1- **Loads** the `PyPD_Metadata_Calculated.CSV` file from the *same* Stage 1  output folder where the `.pkl` file was found. - **Auto-Calculates** 5 time points (t1-t5) for *each* trace by reading its  'PhaseWidth' and 'InterphaseDelay' from the metadata.- **Auto-Calculates** the X-axis plot limits for *each* trace based on its  metadata and defined padding rules.- **Interactive Validation Loop:**  - For each trace, it saves an initial 2-panel plot    (`..._DetectedVals_wOverlay.png`) and its corresponding data    (`..._DetectedVals.CSV`) to the `/PyVS_ExtraInfo_Files/` folder.  - It then displays the plot and prompts the user to [Enter] to accept or to    type in 5 new comma-separated values.  - If new values are entered, it re-generates and re-displays the plot.- **Saves** the final, user-validated 2-panel plot  (`..._SelectedVals_wOverlay.png`) to the main `/PyVS_PNG_Files/` folder.- **Merges** the final, validated voltage data (V1-V5) with the metadata.- **Saves** all final summary files (`.CSV`, `.pkl`) based on the  user-validated data, including maps of all time points and plot limits used.OUTPUTS---------------------------- **`PyVS_VoltageSummary_Final.CSV`**: The main output. A summary table  merging all experiment metadata with the *final* calculated voltage values  (V1-V5, ŒîV, etc.) for every processed file.- **`PyVS_Dictionary_AllDataFiles_SelectedVals.pkl`**: A new pickle file  containing only the *final* 5 selected (t, V, I, RowIndex) points for  each trace.- **`PyVS_Summary.json`**: A JSON file detailing all parameters and files  used for the run, including dictionaries mapping each file to the exact  time points and X-axis limits that were used.- **`/PyVS_PNG_Files/`**: Contains the *final, validated* plots.- **`/PyVS_ExtraInfo_Files/`**: Contains the *initial, auto-detected* plots  and their corresponding data CSVs.  ==============================================================================="""#==============================================================================# üß† AUTO-DETECT SCRIPT NAME & VERSION (from file name)#==============================================================================print("_"*40 + "")import osfrom pathlib import Pathfrom VT_Module_Py_System import get_script_infoscript_name, script_version = get_script_info(__file__)print(f"üü† üßæ Running {script_name} ({script_version})")#==============================================================================# üì¶ IMPORT  #==============================================================================print("üü† üì¶ Importing Packages, Helper Modules...") #--------------------# üì¶ IMPORT REQUIRED PACKAGES#-------------------- # --- Matplotlib backend configuration: prefer Spyder/Jupyter inline ---try:    from IPython import get_ipython    ip = get_ipython()    if ip:  # running inside Spyder/Jupyter        import matplotlib        matplotlib.use("module://matplotlib_inline.backend_inline")    else:        # Not in IPython: only go headless when asked (CI or explicit flag)        import matplotlib        if os.environ.get("CI") or os.environ.get("HEADLESS") == "1":            matplotlib.use("Agg")except Exception:    # If anything fails, let Matplotlib pick a default    passimport sys                          # System exit and interpreter control# --- Math, Data ---import numpy as np                  # Numerical operationsimport pandas as pd                 # DataFrame handling# --- Plotting ---import matplotlibimport matplotlib.pyplot as pltfrom matplotlib import rcParams   # <-- IMPORTED FOR FIGSIZE#--------------------# üõ†Ô∏è IMPORT HELPER MODULES (from .py)#--------------------# --- Settings & Constants ---from VT_Module_Py_GlobalSettings import (    DICT_SYSTEM_INFO,    DICT_TEK_CHANNEL_MAP,    CANON_TIME_UNIT,     xcol_t, ycol_v, ycol_i, # Use aliases for convenience    # rcParams theme applied automatically on import)# --- System Interactions ---from VT_Module_Py_System import (    get_script_info,                # Moved from Utilities    select_pickle_file_gui,       # Moved from FileIO    create_output_structure,        # Moved from Paths    get_parent_data_folder_from_child, # Moved from Paths    STAGE_VOLTAGE_SELECTOR,         # Moved from Paths    print_output_tree,              # Moved from TreeView    print_latest_output_tree,       # Moved from TreeView)# --- Data Input/Output ---from VT_Module_Py_DataIO import (    load_pickle,                    # Moved from FileIO    load_metadata_snapshot,       # Moved from FileIO    save_pipeline_artifacts,    format_metadata_numeric)# --- Data Processing Algorithms ---from VT_Module_Py_Processing import (    standardize_dataframe_to_¬µs,   # Moved from GlobalSettings)# --- Plotting Helpers ---# none used# --- User Input ---# from VT_Module_Py_UserInput import (#     get_user_time_points,     # We now use a local version#     get_time_limits_console,  # We use automated calculation# )#==============================================================================# üõ†Ô∏è ADDITIONAL CONFIGURATION#==============================================================================print("üü† üõ†Ô∏è Applying Additional Configuration...")# Fig Save FormatSAVE_FORMATS = ("png",)# üå≥ TREE VIEW CONFIGSHOW_TREE  = True           # False to disableTREE_MODE  = "current"       # "current", "latest", or "both"# Metadata column aliases for auto-detectionPW_ALIASES = [    "PhaseWidth(¬µs)", "PhaseWidth", "Phase Width", "PW",     "Pulse Width", "PulseWidth", "PhaseWidth(us)"]IP_ALIASES = [    "InterphaseDelay(¬µs)", "InterphaseDelay", "Interphase Delay",     "IPdelay", "Inter phase Delay", "InterphaseDelay(us)"]#==============================================================================# üõ†Ô∏è INTERNAL HELPER FUNCTIONS#==============================================================================def _find_column_alias(df, aliases):    """Finds the first matching column in the DataFrame from a list of aliases."""    df_cols_lower = {col.lower().strip(): col for col in df.columns}    for alias in aliases:        if alias.lower().strip() in df_cols_lower:            return df_cols_lower[alias.lower().strip()]    return Nonedef _extract_values_at_times(df, time_points_¬µs):    """    Extracts V, I, and Row values from a DataFrame at the 5 specified time points.    Includes the local minimum search logic for t3.    """    rows_out = []    has_current = (ycol_i in df.columns)        for t_target in time_points_¬µs:        # Find closest time index        idx_closest = (df[xcol_t] - t_target).abs().idxmin()        row_number  = int(idx_closest) + 1        row         = df.loc[idx_closest]                rows_out.append({            xcol_t:   float(row[xcol_t]),            ycol_v:   float(row[ycol_v]),            ycol_i:   (float(row[ycol_i]) if has_current and pd.notna(row.get(ycol_i)) else np.nan),            "RowIndex": row_number,        })        DF_TEK_SelectedVals = pd.DataFrame(rows_out, columns=[xcol_t, ycol_v, ycol_i, "RowIndex"])        # Ensure canonical columns exist in the right order    for col in [xcol_t, ycol_v, ycol_i, "RowIndex"]:        if col not in DF_TEK_SelectedVals.columns:            DF_TEK_SelectedVals[col] = np.nan    DF_TEK_SelectedVals = DF_TEK_SelectedVals[[xcol_t, ycol_v, ycol_i, "RowIndex"]]        #--------------------    # üîç Improve v3: Local minimum tweak near ¬±10 ¬µs    #--------------------    delta_¬µs = 10       # Search ¬±10 ¬µs around t3    t3 = time_points_¬µs[2]        # Create mask for time window    mask = (df[xcol_t] >= t3 - delta_¬µs) & (df[xcol_t] <= t3 + delta_¬µs)    DF_window = df[mask]        if not DF_window.empty:        # Find row with minimum voltage        idx_min    = DF_window[ycol_v].idxmin()        row        = df.loc[idx_min]        row_number = int(idx_min) + 1                # Assign by label and include ALL columns to avoid NaNs        DF_TEK_SelectedVals.loc[            DF_TEK_SelectedVals.index[2],            [xcol_t, ycol_v, ycol_i, "RowIndex"]        ] = [            float(row[xcol_t]),            float(row[ycol_v]),            (float(row[ycol_i]) if (ycol_i in df.columns and pd.notna(row.get(ycol_i))) else np.nan),            row_number,        ]        # Make RowIndex a proper integer type (nullable) for clean CSVs    DF_TEK_SelectedVals["RowIndex"] = DF_TEK_SelectedVals["RowIndex"].astype("Int64")        return DF_TEK_SelectedValsdef _create_plot(key, df, DF_TEK_SelectedVals, x_lims):    """    Generates a 2-panel V/I plot with scatter overlay.    Returns the 'fig' object.    """    xmin_t, xmax_t = x_lims        # --- Check if current data exists ---    has_current = (ycol_i in df.columns) and (not df[ycol_i].isnull().all())        # --- Create 2-panel figure ---    fig, (ax_v, ax_i) = plt.subplots(        2,         1,         sharex=True,        figsize=(rcParams["figure.figsize"][0], rcParams["figure.figsize"][1] * 1.2)    )        # === Figure Title ===    fig.suptitle(f"{key} ‚Äî Voltage & Current vs Time")        # === TOP PANEL (VOLTAGE) ===        # --- Plot line trace ---    ax_v.plot(        df[xcol_t],        df[ycol_v],        label="Full Trace",        linewidth=1.2,        alpha=1.0,        color="#E6B800",    # Amber (from PyPD style)    )        # --- Overlay 5 scatter points ---    ax_v.scatter(        DF_TEK_SelectedVals[xcol_t],        DF_TEK_SelectedVals[ycol_v],        label="Selected Values",        s=7,         zorder=5,        color='black',    )        # --- Format Top Panel ---    ax_v.set_ylabel(ycol_v)    ax_v.legend(loc="upper left")    # Hide x-tick labels on the top plot    ax_v.tick_params(axis="x", which="both", labelbottom=False)            # === BOTTOM PANEL (CURRENT) ===        if has_current:        # --- Plot line trace ---        ax_i.plot(            df[xcol_t],            df[ycol_i],            label="Full Trace",            linewidth=1.2,            alpha=1.0,            color="#1E90FF",    # Dodger Blue (from PyPD style)        )                # --- Overlay 5 scatter points ---        ax_i.scatter(            DF_TEK_SelectedVals[xcol_t],            DF_TEK_SelectedVals[ycol_i],            label="Selected Values",            s=7,             zorder=5,            color='black',        )        ax_i.legend(loc="upper left")            else:        # Show placeholder text if no current data        ax_i.text(            0.5, 0.5, "Current data not available",            horizontalalignment='center',            verticalalignment='center',            transform=ax_i.transAxes,            color='gray'        )        # --- Format Bottom Panel ---    ax_i.set_xlabel(xcol_t)    ax_i.set_ylabel(ycol_i)    ax_i.set_xlim(xmin_t, xmax_t)        # --- Final Figure-level Formatting ---    fig.align_ylabels([ax_v, ax_i])    # (Removed fig.tight_layout() to match PyPD behavior)        return figdef _parse_custom_times(input_str, default_list):    """    Parses a comma-separated string into a list of 5 floats.    Returns the default_list if parsing fails.    """    try:        parts = input_str.split(',')        if len(parts) != 5:            raise ValueError("Input must have exactly 5 values.")                time_points = [float(t.strip()) for t in parts]        print(f"   ‚úÖ Custom values: {time_points}")        return time_points        except Exception as e:        print(f"   üö´ Invalid input: {e}. Reverting to default values.")        return default_list#==============================================================================# üöÄ MAIN SCRIPT WORKFLOW#==============================================================================def main():    """Runs the full Stage 2 (VoltageSelector) pipeline.    This function orchestrates the entire Stage 2 workflow:    1. Prompts user to select the Stage 1 `.pkl` output file.    2. Creates the standardized Stage 2 output folder structure (including       the 'ExtraInfo' folder via the modified helper module).    3. Loads the pickled DataFrame dictionary and the Stage 1 Metadata.    4. Auto-calculates t1-t5 for all traces based on metadata.    5. Auto-calculates file-specific X-axis plot limits based on metadata       (PW, IP) and padding rules.    6. Begins an interactive loop for each file:       a. Generates, saves, and displays the "Detected" plot/CSV to the          ExtraInfo folder.       b. Prompts user to accept or enter custom time points.       c. If custom points are entered, re-generates and re-displays the plot.       d. Stores the final, validated points and plot for export.    7. Merges the final validated data with the metadata into `DF_Final`.    8. Assembles and saves all artifacts (CSVs, pickles, plots, summaries)        using a single call to `save_pipeline_artifacts`.    9. Displays the final folder tree.    """        #====================    # üìÅ GUI File Picker - LOAD SAVED PKL DICTIONARY (.pkl)    #====================     # ‚≠ïÔ∏è [Helper fn] imported from  VT_Module_Py_FileIO.py    # üì£ [Call fn]        #--------------------    # üìÅ PATHS FROM SELECTED PKL    #--------------------    file_path_pkl = Path(select_pickle_file_gui())   # pick Stage 1 .pkl once    stage1_folder = file_path_pkl.parent        # EXPERIMENT ROOT (via System helper you patched): the folder that contains VT_Py_Outputs    folder_path_parent = get_parent_data_folder_from_child(file_path_pkl)    print(f"\n‚Ü≥ üìÇ Original Parent Folder (for creating Stage 2 folder): {folder_path_parent}")        # Create Stage 2 folder structure (let System helper do ALL the directory creation)    stage_name, stage_id = STAGE_VOLTAGE_SELECTOR    paths = create_output_structure(folder_path_parent, stage_name, stage_id)        print(f"\n‚úÖ Created Output Folders:"          f"\n‚Ü≥üìÇ {paths.output.name}"          f"\n   ‚Ü≥üìÇ {paths.stage.name}"          f"\n      ‚Ü≥üìÇ {paths.processed.name}"          f"\n         ‚Ü≥üìÇ {paths.csv.name}"          f"\n         ‚Ü≥üìÇ {paths.png.name}"        + (f"\n         ‚Ü≥üìÇ {paths.extra.name}" if paths.extra else "")        + (f"\n         ‚Ü≥üìÇ {paths.svg.name}"  if paths.svg  else "")          )        #--------------------    # üìÑ METADATA CSV (robust discovery next to the .pkl)    #--------------------        # Prioritize the 24-column 'Calculated' file.    meta_candidates = [        stage1_folder / "PyPD_Metadata_Calculated.CSV",        stage1_folder / "PyPD_Metadata_Loaded.CSV",    ]    meta_loaded_path = next((p for p in meta_candidates if p.exists()), None)    if meta_loaded_path is None:        print("‚ùå [PyVS] CRITICAL ERROR: No Stage 1 metadata CSV found next to the .pkl.")        print("   Looked for:", ", ".join(p.name for p in meta_candidates))        raise SystemExit("Exiting due to missing metadata.")    #---------------------------    # üì§ LOAD PICKLE    #---------------------------    # ‚≠ïÔ∏è [Helper fn] imported from  VT_Module_Py_FileIO.py    # üì£ [Call fn]    try:        DICT_DF_TEK_Loaded = load_pickle(file_path_pkl)        # --- CHECK IF STANDARDIZATION IS NEEDED ---        is_standardized = False        if DICT_DF_TEK_Loaded:            try:                # Get the first item's DataFrame to check its columns                first_key = next(iter(DICT_DF_TEK_Loaded))                first_df = DICT_DF_TEK_Loaded[first_key]                # Check if the canonical columns are already present                if (xcol_t in first_df.columns and ycol_v in first_df.columns):                    is_standardized = True            except Exception as e_check:                print(f"   üö´ Warning: Could not check standardization status: {e_check}. Assuming no.")                is_standardized = False                if is_standardized:            print("\n   ‚úÖ Loaded DataFrames are already standardized. Skipping re-standardization.")        else:            # --- RUN STANDARDIZATION CALL ---            print("\nüü† Standardizing loaded DataFrames to ¬µs/V/A...")            DICT_DF_TEK_Standardized = {}            standardization_errors = 0            for key, df_loaded in DICT_DF_TEK_Loaded.items():                try:                    # Use the imported function                    DICT_DF_TEK_Standardized[key] = standardize_dataframe_to_¬µs(                        df_loaded,                         channel_map=DICT_TEK_CHANNEL_MAP # Now this import is used                    )                except Exception as e_std:                    print(f"   üö´ Skipping {key} due to standardization error: {e_std}")                    standardization_errors += 1                    DICT_DF_TEK_Loaded = DICT_DF_TEK_Standardized # Overwrite with standardized data            if standardization_errors > 0:                 print(f"   üö´ {standardization_errors} file(s) skipped during standardization.")                # --- FINAL CHECK (runs for both cases) ---        if not DICT_DF_TEK_Loaded:             print("üö´ No traces found or remain after loading. Check input PKL file contents.")             sys.exit(1)        # --- END MODIFIED BLOCK ---    except Exception as e:        print(f"üö´ Error loading or standardizing .pkl file: {e}")        sys.exit()    # =============================================================================    # üìö LOAD METADATA & AUTO-CALCULATE TIME POINTS    # =============================================================================        # Required base columns (24)    META_COLS = [        "FileID", "Date", "WaferID", "DeviceID", "ElectrodeID",         "ElectrodeGeometry", "ElectrodeDiameter(cm)", "ElectrodeGSA(cm^2)",        "ElectrodeMaterial", "Electrolyte", "TestInfo1", "TestInfo2",        "WaveformID", "Current(¬µA)", "PhaseWidth(¬µs)", "InterphaseDelay(¬µs)",        "Frequency(Hz)", "ChargePerPhase(pC)", "ChargePerPhase(nC)",        "ChargePerPhase(mC)", "ChargePerPhase(C)",         "ChargeDensityPerPhase(mC/cm^2)", "OtherInfo", "TotalPulses",    ]        # ---- 1) Load PyPD_Metadata_Loaded.CSV from the Stage 1 output folder ----    DF_Metadata_Reference = load_metadata_snapshot(meta_loaded_path, META_COLS)    if DF_Metadata_Reference is None:        print("‚ùå [PyVS] CRITICAL ERROR: Could not load metadata from Stage 1.")        print(f"   Could not find a metadata CSV in: {meta_loaded_path.name}")        print("   Please re-run Stage 1 and ensure 'Metadata.xlsx' is found.")        raise SystemExit("Exiting due to missing metadata.")    print("   ‚úÖ [PyVS] Successfully loaded Stage 1 metadata.")    # ---- 2) Find the PW and IP columns in the loaded metadata ----    pw_col = _find_column_alias(DF_Metadata_Reference, PW_ALIASES)    ip_col = _find_column_alias(DF_Metadata_Reference, IP_ALIASES)    if not pw_col:        print(f"   üö´ Warning: Could not find Phase Width column. Searched for: {PW_ALIASES}")    if not ip_col:        print(f"   üö´ Warning: Could not find Interphase Delay column. Searched for: {IP_ALIASES}")    # ---- 3) Build a lookup map of FileID -> {PW, IP} ----    metadata_map = {}    default_pw = 200.0 # Fallback values    default_ip = 100.0        # Normalize FileID for lookup    def _norm(s): return Path(str(s)).stem.strip()        for index, row in DF_Metadata_Reference.iterrows():        try:            file_id = _norm(row["FileID"])                        pw = pd.to_numeric(row.get(pw_col), errors='coerce')            ip = pd.to_numeric(row.get(ip_col), errors='coerce')                        metadata_map[file_id] = {                "PW": float(pw) if pd.notna(pw) else default_pw,                "IP": float(ip) if pd.notna(ip) else default_ip,            }        except Exception:            pass # Skip rows that fail                # ---- 4) Auto-Calculate t1-t5 for all files ----    print("\nüü† Auto-calculating t1-t5 from metadata...")    auto_time_points_map = {}    default_time_points_¬µs = [-25.0, 1.0, 198.0, 201.0, 292.0] # Fallback        for key in DICT_DF_TEK_Loaded.keys():        meta = metadata_map.get(_norm(key))        if meta:            PW = meta["PW"]            IP = meta["IP"]            t1 = -25.0            t2 = 1.0            t3 = PW - 2.0            t4 = PW + 1.0            t5 = (PW + IP) - 8.0            auto_time_points_map[key] = [t1, t2, t3, t4, t5]            print(f"   ‚úÖ {key}: PW={PW}, IP={IP} -> {auto_time_points_map[key]}")        else:            auto_time_points_map[key] = default_time_points_¬µs            print(f"   üö´ {key}: Metadata not found. Using defaults: {default_time_points_¬µs}")        # =============================================================================    # üö∏ INTERACTIVE VALIDATION LOOP    # =============================================================================    print("\n" + "="*50)    print("üöÄ STARTING INTERACTIVE VALIDATION üöÄ")    print("Please follow these steps for each file:")    print("  1. üñºÔ∏è  An initial plot will appear with auto-detected points.")    print("  2. ‚ùóÔ∏è Close the plot window to continue to the prompt.")    print("  3. üëÄ Review the 'POINTS' printed in the console.")    print("  4. üëâ Press [Enter] to accept, or type 5 new values in {CANON_TIME_UNIT}  (e.g., -25,1,198,201,292)")     print("\n   (Initial plots and data are saved to the 'PyVS_ExtraInfo_Files' folder.)")    print("="*50)            DICT_DF_TEK_SelectedVals = {} # This will store the FINAL validated data    DICT_DF_TEK_DetectedVals = {} # <-- NEW: Store auto-detected dataframes    figures_to_save = {}      # This will store the FINAL validated plots    final_time_points_map = {}  # Store final t1-t5 for summary    final_x_lims_map = {}       # Store final x-lims for summary        for key, df in DICT_DF_TEK_Loaded.items():        # --- Check for empty DataFrame ---        if df.empty:            print("\n" + "="*50)            print(f"FILE:   {key}")            print("üö´ Skipping. Contains no data.")                        # Add placeholder entries to prevent errors during final summary            final_time_points_map[key] = [] # Use empty list            final_x_lims_map[key] = [None, None] # Use [None, None]                        # Add an empty DataFrame to the final dictionaries            empty_df = pd.DataFrame(columns=[xcol_t, ycol_v, ycol_i, "RowIndex"])            DICT_DF_TEK_SelectedVals[key] = empty_df            DICT_DF_TEK_DetectedVals[key] = empty_df                        continue # Skip to the next file                # --- Auto-calculate X-axis limits ---        meta = metadata_map.get(_norm(key))        if meta:            PW = meta["PW"]            IP = meta["IP"]        else:            PW = 200.0 # Default fallback            IP = 100.0 # Default fallback                    t0 = 0.0        t_end_ideal = t0 + PW + IP + PW  # Ideal end of anodal phase        t_padded_start = t0 - 200.0      # Per user: 200us pad before        t_padded_end = t_end_ideal + 200.0 # Per user: 200us pad after                xmin_t = t_padded_start - 100.0  # Per user: 100us extra margin        xmax_t = t_padded_end + 100.0    # Per user: 100us extra margin        plot_x_lims = (xmin_t, xmax_t)        final_x_lims_map[key] = plot_x_lims # STORE FOR SUMMARY                # --- 1. Get Auto-Detected Values & Plot ---        auto_time_points = auto_time_points_map[key]        DF_TEK_DetectedVals = _extract_values_at_times(df, auto_time_points)        DICT_DF_TEK_DetectedVals[key] = DF_TEK_DetectedVals # <-- STORE DETECTED DF                # --- 2. Create, Save, and Show "Detected" Plot ---        fig_detected = _create_plot(key, df, DF_TEK_DetectedVals, plot_x_lims)                file_base_detected_plot = f"PyVS_{key}_DetectedVals_wOverlay"        file_base_detected_csv = f"PyVS_{key}_DetectedVals.CSV"                # --- Save to EXTRA folder (check if paths.extra exists) ---        if paths.extra:            try:                # Save the Detected Plot                for ext in SAVE_FORMATS:                    out_fig_path = paths.extra / f"{file_base_detected_plot}.{ext}"                    fig_detected.savefig(out_fig_path)                                # Save the Detected CSV                out_csv_path = paths.extra / file_base_detected_csv                DF_TEK_DetectedVals.to_csv(out_csv_path, index=False)                            except Exception as e_save:                print(f"   üö´ Could not save detected files to extra folder: {e_save}")        else:            # This should not happen if VT_Module_Py_System.py is modified,            # but it's a safe fallback.            print(f"   üö´ 'ExtraInfo' folder not found {paths.extra} is None).")            print("   üö´ Please modify VT_Module_Py_System.py to create it for 'VS' stage.")            print("   üö´ Skipping save for detected files.")                # Show the plot (blocks until user closes it)        plt.show()                 # --- 3. Prompt for Validation ---        print("\n" + "="*50)        print(f"üî∏ FILE:   {key}")        print(f"POINTS ({CANON_TIME_UNIT}): {auto_time_points}")                prompt = "   > [Enter] / Type 5 values: "        user_input = input(prompt)                # --- 4. Process Input ---        file_base_selected_plot = f"PyVS_{key}_SelectedVals_wOverlay"                if user_input.strip() == "":            # User Accepted            print("   ‚úÖ Accepted.")            final_time_points = auto_time_points            DF_TEK_SelectedVals = DF_TEK_DetectedVals                        # We already have the figure, just add it to the final save list            figures_to_save[file_base_selected_plot] = fig_detected                    else:            # User provided custom values            plt.close(fig_detected) # Close the old plot                        final_time_points = _parse_custom_times(user_input, auto_time_points)            DF_TEK_SelectedVals = _extract_values_at_times(df, final_time_points)                        # Re-plot and show the new "Final" plot            fig_selected = _create_plot(key, df, DF_TEK_SelectedVals, plot_x_lims)                        print("   ... Displaying new plot (close window to continue) ...")            plt.show() # Blocks until user closes                        # Add the new figure to the final save list            figures_to_save[file_base_selected_plot] = fig_selected                    # Store the final, validated DataFrame        DICT_DF_TEK_SelectedVals[key] = DF_TEK_SelectedVals        final_time_points_map[key] = final_time_points # STORE FOR SUMMARY            print("\n" + "="*50)    print("‚úÖ Interactive validation complete.")    #==============================================================================    # üìö BUILD FINAL SUMMARY (base metadata + computed V columns)    # =============================================================================        # We already loaded DF_Metadata_Reference, now we just use it        # ---- 2) Build computed voltage columns from DICT_DF_TEK_SelectedVals ----         def _get_val(df, i, col):        """Safely extracts a single value from a DataFrame by iloc."""        try:            v = df.iloc[i][col]            return float(v) if pd.notna(v) else np.nan        except Exception:            return np.nan                # Build a small table keyed by FileID    calc_rows = []    # --- MODIFIED: Use the FINAL validated dictionary ---    for key, df_sel in DICT_DF_TEK_SelectedVals.items():        # --- NEW: Handle empty df for skipped files ---        if df_sel.empty:            v1, v2, v3, v4, v5 = (np.nan,) * 5        else:            v1 = _get_val(df_sel, 0, ycol_v)            v2 = _get_val(df_sel, 1, ycol_v)            v3 = _get_val(df_sel, 2, ycol_v)            v4 = _get_val(df_sel, 3, ycol_v)            v5 = _get_val(df_sel, 4, ycol_v)                calc_rows.append({            "FileID": key,                          # join key            "Eipp (V1)": v1,            "V2": v2,            "Vc peak (V3)": v3,            "Emc (V4)": v4,            "E IPend (v5)": v5,                     # kept as requested (lowercase v5)            "Vacc leading (V2=V1)": v1,            "ŒîV (V3-V1)": (np.nan if (np.isnan(v3) or np.isnan(v1)) else (v3 - v1)),            "ŒîEp (V3-V2)": (np.nan if (np.isnan(v3) or np.isnan(v2)) else (v3 - v2)),            "Vacc trailing (V4=V3)": v3,        })            DF_Computed = pd.DataFrame(calc_rows)        # Normalize FileIDs for merging    DF_Metadata_Reference["FileID"] = DF_Metadata_Reference["FileID"].map(_norm)    DF_Computed["FileID"] = DF_Computed["FileID"].map(_norm)        # ---- 3) Merge: base metadata (left) + computed voltages on FileID ----    DF_Final = DF_Metadata_Reference.merge(DF_Computed, on="FileID", how="left")        print("\nüü† üì§ Exporting Summaries")    # ---- 4A) Export loaded Metadata (DF_Metadata_Reference) ----    meta_copy_name = "PyVS_Metadata_Reference.CSV"    # meta_copy_path = paths.csv / meta_copy_name     print(f"‚úÖ üíæ Saved {meta_copy_name} (Snapshot copy)")        # ---- 4B) Export computed voltages (DF_Computed) ----    computed_csv_name = "PyVS_VoltageSummary_CalculatedVals.CSV"    # computed_csv_path = paths.csv / computed_csv_name    print(f"‚úÖ üíæ Saved {computed_csv_name} ({len(DF_Computed)} rows)")        # ---- 4C) Export merged summary (DF_Final) ----    summary_csv_name = "PyVS_VoltageSummary_Final.CSV"    summary_csv_path = paths.stage / summary_csv_name    # --- Apply numeric formatting to the final merged summary only ---    DF_Final = format_metadata_numeric(DF_Final)    summary_csv_name = "PyVS_VoltageSummary_Final.CSV"    summary_csv_path = paths.stage / summary_csv_name        DF_Final.to_csv(summary_csv_path, index=False)    print(f"‚úÖ üíæ Saved Final Summary: {summary_csv_name} ({len(DF_Final)} rows)")        # =========================================================================    # üì§ EXPORT FINAL ARTIFACTS    # =========================================================================    print("\nüü† üì§ Preparing final artifacts for export...")    try:        # 1. Define what to save                # --- Selected (Final) Plots ---        # 'figures_to_save' now contains all the FINAL validated plots        try:            selected_plots = [f"{base}.{ext}" for base in figures_to_save.keys() for ext in SAVE_FORMATS]        except Exception:            selected_plots = [] # Fallback                    # --- Detected (Initial) Plots (Find them in the extra folder) ---        try:            if paths.extra:                detected_plots = [f for f in os.listdir(paths.extra) if f.endswith("_DetectedVals_wOverlay.png")]            else:                detected_plots = []        except Exception:            detected_plots = []                    all_plot_filenames = selected_plots + detected_plots            # --- Per-file CSVs (Reference & Calculated) ---        dataframes_csv = {            "PyVS_Metadata_Reference.CSV": DF_Metadata_Reference,            "PyVS_VoltageSummary_CalculatedVals.CSV": DF_Computed        }                # --- Detected DataFrames (to Extra folder) ---        # This dict will be passed to save_pipeline_artifacts        dataframes_extra = {            f"PyVS_{key}_DetectedVals.CSV": df            for key, df in DICT_DF_TEK_DetectedVals.items() if not df.empty        }                # --- Pickle to save (goes to stage folder) ---        pkl_out_name = "PyVS_Dictionary_AllDataFiles_SelectedVals.pkl"        pickles_to_save = {            pkl_out_name: DICT_DF_TEK_SelectedVals # This is the final, validated dict        }            # --- Final Summary CSV (already saved, just list name) ---        summary_csv_name = "PyVS_VoltageSummary_Final.CSV" # Saved earlier            # 2. Assemble PyVS_Summary dictionary        PyVS_Summary = {            "timestamp": paths.timestamp,            "script": {"name": script_name, "version": script_version},            "system": DICT_SYSTEM_INFO,            "input_file_pkl": str(file_path_pkl),              "input_file_metadata": str(meta_loaded_path),            "output_folder": str(paths.stage),             "n_files_processed": len(DICT_DF_TEK_Loaded),            "file_keys": list(DICT_DF_TEK_Loaded.keys()),            "user_time_points_¬µs": final_time_points_map,            "user_time_limits_¬µs": final_x_lims_map,            "artifacts": {                "metadata_csv": meta_copy_name,                 "calculated_csv": computed_csv_name,                "summary_csv": summary_csv_name,                "plots": sorted(list(set(all_plot_filenames))), # <-- Correctly lists all plots                "detected_csvs": list(dataframes_extra.keys()), # <-- Correctly lists new CSVs                "pickle": pkl_out_name,            }        }        # 3. Make the single call to the exporter        save_pipeline_artifacts(            paths=paths,            summary_data=PyVS_Summary,            summary_base_name="PyVS_Summary",            figures_to_save=figures_to_save, # Saves the FINAL plots            pickles_stage=pickles_to_save        )        except Exception as e:        print(f"\nüö´ FAILED TO EXPORT ARTIFACTS: {e}")            #=====================     # üå≥ OUTPUT FOLDER TREE (optional)    #=====================    if SHOW_TREE:        try:            if TREE_MODE in ("current", "both"):                print("\nüå≥ Folder Tree:")                print_output_tree(paths.stage)            if TREE_MODE in ("latest", "both"):                print_latest_output_tree(paths.output, stage_name=stage_name)        except Exception as e:            print(f"üö´ Folder tree preview failed: {e}")    #=====================    # üñºÔ∏è Show Plots (if any)    #=====================    # plt.show() is now handled INTERACTIVELY inside the loop    print("\n(Interactive plotting complete)")        # =============================================================================# üöÄ Run# =============================================================================if __name__ == "__main__":    main()# =============================================================================# Code Complete# =============================================================================print("_"*40 + "")print(f"üéâ Completed: {script_name}")print("_"*40 + "")